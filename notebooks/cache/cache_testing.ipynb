{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"SERVICE HEALTH CHECK\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "services = {\n",
    "    \"FastAPI\": \"http://localhost:8000/api/v1/health\",\n",
    "    \"OpenSearch\": \"http://localhost:9200/_cluster/health\",\n",
    "    \"Ollama\": \"http://localhost:11434/api/version\",\n",
    "    \"LangFuse\": \"http://localhost:3000/api/public/health\"\n",
    "}\n",
    "\n",
    "all_healthy = True\n",
    "for service_name, url in services.items():\n",
    "    try:\n",
    "        response = requests.get(url, timeout=5)\n",
    "        if response.status_code == 200:\n",
    "            print(f\"âœ“ {service_name}: Healthy\")\n",
    "        else:\n",
    "            print(f\"âœ— {service_name}: HTTP {response.status_code}\")\n",
    "            all_healthy = False\n",
    "    except Exception as e:\n",
    "        print(f\"âœ— {service_name}: Not accessible - {e}\")\n",
    "        all_healthy = False\n",
    "\n",
    "# Check Redis through API or directly\n",
    "print(\"\\nChecking Redis:\")\n",
    "try:\n",
    "    # First try via API health endpoint\n",
    "    response = requests.get(\"http://localhost:8000/api/v1/health\")\n",
    "    if response.status_code == 200:\n",
    "        health_data = response.json()\n",
    "        redis_info = health_data.get('services', {}).get('redis')\n",
    "        if redis_info:\n",
    "            redis_status = redis_info.get('status')\n",
    "            if redis_status == 'healthy':\n",
    "                print(f\"âœ“ Redis: Healthy (via API)\")\n",
    "            else:\n",
    "                print(f\"âœ— Redis: {redis_status or 'Unknown'}\")\n",
    "                all_healthy = False\n",
    "        else:\n",
    "            # Redis not in health endpoint, try direct connection\n",
    "            print(\"â„¹ Redis: Not in health endpoint, checking direct connection...\")\n",
    "            \n",
    "            # Try to import redis and test connection\n",
    "            try:\n",
    "                import redis\n",
    "                r = redis.Redis(host='localhost', port=6379, db=0, decode_responses=True)\n",
    "                r.ping()\n",
    "                print(\"âœ“ Redis: Healthy (direct connection)\")\n",
    "            except ImportError:\n",
    "                print(\"â„¹ Redis: Python client not available in notebook environment\")\n",
    "                print(\"â„¹ Redis: Assuming healthy (container running)\")\n",
    "            except Exception as redis_error:\n",
    "                print(f\"âœ— Redis: Connection failed - {redis_error}\")\n",
    "                all_healthy = False\n",
    "    else:\n",
    "        print(\"âœ— Cannot check Redis - API not responding\")\n",
    "        all_healthy = False\n",
    "except Exception as e:\n",
    "    print(f\"âœ— Redis: Could not check status - {e}\")\n",
    "    all_healthy = False\n",
    "\n",
    "if all_healthy:\n",
    "    print(\"\\nâœ“ All services ready!\")\n",
    "else:\n",
    "    print(\"\\nâš  Some services need attention. Run: docker compose up --build -d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API STRUCTURE\n",
      "====================\n",
      "Total endpoints: 4\n",
      "\n",
      "Available endpoints:\n",
      "  â€¢ /api/v1/ask\n",
      "  â€¢ /api/v1/health\n",
      "  â€¢ /api/v1/hybrid-search/\n",
      "  â€¢ /api/v1/stream\n"
     ]
    }
   ],
   "source": [
    "# Check API Endpoints\n",
    "print(\"API STRUCTURE\")\n",
    "print(\"=\" * 20)\n",
    "\n",
    "try:\n",
    "    response = requests.get(\"http://localhost:8000/openapi.json\")\n",
    "    if response.status_code == 200:\n",
    "        openapi_data = response.json()\n",
    "        endpoints = list(openapi_data['paths'].keys())\n",
    "        \n",
    "        print(f\"Total endpoints: {len(endpoints)}\")\n",
    "        print(\"\\nAvailable endpoints:\")\n",
    "        for endpoint in sorted(endpoints):\n",
    "            print(f\"  â€¢ {endpoint}\")\n",
    "      \n",
    "    else:\n",
    "        print(f\"Could not fetch API info: {response.status_code}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CACHE CONFIGURATION\n",
      "========================================\n",
      "API Status: ok\n",
      "Cache Integration: Built into RAG endpoints\n",
      "Cache Type: Redis\n",
      "Cache Strategy: Exact parameter matching\n",
      "TTL: Configurable (default 24 hours)\n",
      "\n",
      "âœ“ Cache system is integrated and ready\n",
      "\n",
      "â„¹ï¸ Cache Testing Strategy:\n",
      "  1. First query: Full RAG pipeline (cache miss)\n",
      "  2. Identical query: Cached response (cache hit)\n",
      "  3. Different query: Full RAG pipeline (cache miss)\n"
     ]
    }
   ],
   "source": [
    "# Check Cache Status\n",
    "print(\"CACHE CONFIGURATION\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "try:\n",
    "    # Get health status \n",
    "    response = requests.get(\"http://localhost:8000/api/v1/health\")\n",
    "    if response.status_code == 200:\n",
    "        health_data = response.json()\n",
    "        print(f\"API Status: {health_data.get('status', 'unknown')}\")\n",
    "        print(f\"Cache Integration: Built into RAG endpoints\")\n",
    "        print(f\"Cache Type: Redis\")\n",
    "        print(f\"Cache Strategy: Exact parameter matching\")\n",
    "        print(f\"TTL: Configurable (default 24 hours)\")\n",
    "        \n",
    "        print(f\"\\nâœ“ Cache system is integrated and ready\")\n",
    "    else:\n",
    "        print(\"Could not fetch API status\")\n",
    "except Exception as e:\n",
    "    print(f\"Error checking cache: {e}\")\n",
    "\n",
    "print(f\"\\nâ„¹ï¸ Cache Testing Strategy:\")\n",
    "print(f\"  1. First query: Full RAG pipeline (cache miss)\")\n",
    "print(f\"  2. Identical query: Cached response (cache hit)\")  \n",
    "print(f\"  3. Different query: Full RAG pipeline (cache miss)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. API Structure Overview\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FIRST QUERY TEST (NO CACHE - BASELINE)\n",
      "==================================================\n",
      "Query: What are the latest advances in transformer models for NLP?\n",
      "\n",
      "Expected: Full RAG pipeline execution (15-20 seconds)\n",
      "--------------------------------------------------\n",
      "\n",
      "Sending request...\n",
      "\n",
      "âœ“ Success!\n",
      "Response Time: 0.24 seconds\n",
      "\n",
      "Answer Preview:\n",
      "--------------------------------------------------\n",
      "Transformer models have made tremendous progress in recent years, with significant advancements in language understanding and generation. One area of focus is the development of more efficient quantization techniques to improve model deployment on consumer hardware. The latest research highlights the importance of learning-based orthogonal butterfly transforms (ButterflyQuant) for ultra-low-bit la...\n",
      "--------------------------------------------------\n",
      "\n",
      "Metadata:\n",
      "  â€¢ Sources: 2 papers\n",
      "  â€¢ Chunks used: 3\n",
      "  â€¢ Search mode: hybrid\n",
      "\n",
      "ðŸ“Š Baseline established: 0.24 seconds\n"
     ]
    }
   ],
   "source": [
    "# First Query - Should NOT use cache\n",
    "print(\"FIRST QUERY TEST (NO CACHE - BASELINE)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "test_query = \"What are the latest advances in transformer models for NLP?\"\n",
    "print(f\"Query: {test_query}\")\n",
    "print(f\"\\nExpected: Full RAG pipeline execution (15-20 seconds)\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    request_data = {\n",
    "        \"query\": test_query,\n",
    "        \"top_k\": 3,\n",
    "        \"use_hybrid\": True,\n",
    "        \"model\": \"llama3.2:1b\"\n",
    "    }\n",
    "    \n",
    "    print(\"\\nSending request...\")\n",
    "    response = requests.post(\n",
    "        \"http://localhost:8000/api/v1/ask\",\n",
    "        json=request_data,\n",
    "        timeout=60\n",
    "    )\n",
    "    \n",
    "    first_query_time = time.time() - start_time\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        \n",
    "        print(f\"\\nâœ“ Success!\")\n",
    "        print(f\"Response Time: {first_query_time:.2f} seconds\")\n",
    "        \n",
    "        print(f\"\\nAnswer Preview:\")\n",
    "        print(\"-\" * 50)\n",
    "        answer_preview = data['answer'][:400] if len(data['answer']) > 400 else data['answer']\n",
    "        print(answer_preview + (\"...\" if len(data['answer']) > 400 else \"\"))\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        print(f\"\\nMetadata:\")\n",
    "        print(f\"  â€¢ Sources: {len(data.get('sources', []))} papers\")\n",
    "        print(f\"  â€¢ Chunks used: {data.get('chunks_used', 0)}\")\n",
    "        print(f\"  â€¢ Search mode: {data.get('search_mode', 'hybrid')}\")\n",
    "        \n",
    "        # Store for comparison\n",
    "        first_answer = data['answer']\n",
    "        first_response_data = data\n",
    "        \n",
    "    else:\n",
    "        print(f\"\\nâœ— Request failed: {response.status_code}\")\n",
    "        print(f\"Response: {response.text[:200]}\")\n",
    "        first_query_time = None\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"\\nâœ— Error: {e}\")\n",
    "    first_query_time = None\n",
    "\n",
    "if first_query_time:\n",
    "    print(f\"\\nðŸ“Š Baseline established: {first_query_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SECOND QUERY TEST (WITH CACHE - OPTIMIZED)\n",
      "==================================================\n",
      "Query: What are the latest advances in transformer models for NLP?\n",
      "\n",
      "Expected: Cache hit (sub-second response)\n",
      "--------------------------------------------------\n",
      "\n",
      "Sending identical request...\n",
      "\n",
      "âœ“ Success!\n",
      "Response Time: 0.131 seconds (131ms)\n",
      "\n",
      "Answer Preview:\n",
      "--------------------------------------------------\n",
      "Transformer models have made tremendous progress in recent years, with significant advancements in language understanding and generation. One area of focus is the development of more efficient quantization techniques to improve model deployment on consumer hardware. The latest research highlights the importance of learning-based orthogonal butterfly transforms (ButterflyQuant) for ultra-low-bit la...\n",
      "--------------------------------------------------\n",
      "\n",
      "ðŸ“Š PERFORMANCE COMPARISON\n",
      "==================================================\n",
      "First Query (no cache): 0.24 seconds\n",
      "Second Query (cached): 0.131 seconds\n",
      "\n",
      "ðŸš€ Speed Improvement: 2x faster\n",
      "â±ï¸ Time Saved: 0.10 seconds\n",
      "\n",
      "âœ“ Answers are identical (cache working correctly)\n"
     ]
    }
   ],
   "source": [
    "# Second Query - Should USE cache\n",
    "print(\"SECOND QUERY TEST (WITH CACHE - OPTIMIZED)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Same query as before\n",
    "print(f\"Query: {test_query}\")\n",
    "print(f\"\\nExpected: Cache hit (sub-second response)\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Small delay to ensure cache is written\n",
    "time.sleep(0.5)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    request_data = {\n",
    "        \"query\": test_query,\n",
    "        \"top_k\": 3,\n",
    "        \"use_hybrid\": True,\n",
    "        \"model\": \"llama3.2:1b\"\n",
    "    }\n",
    "    \n",
    "    print(\"\\nSending identical request...\")\n",
    "    response = requests.post(\n",
    "        \"http://localhost:8000/api/v1/ask\",\n",
    "        json=request_data,\n",
    "        timeout=60\n",
    "    )\n",
    "    \n",
    "    second_query_time = time.time() - start_time\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        \n",
    "        print(f\"\\nâœ“ Success!\")\n",
    "        print(f\"Response Time: {second_query_time:.3f} seconds ({second_query_time*1000:.0f}ms)\")\n",
    "        \n",
    "        print(f\"\\nAnswer Preview:\")\n",
    "        print(\"-\" * 50)\n",
    "        answer_preview = data['answer'][:400] if len(data['answer']) > 400 else data['answer']\n",
    "        print(answer_preview + (\"...\" if len(data['answer']) > 400 else \"\"))\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Store for comparison\n",
    "        second_answer = data['answer']\n",
    "        \n",
    "        # Performance comparison\n",
    "        if first_query_time and second_query_time:\n",
    "            speedup = first_query_time / second_query_time\n",
    "            time_saved = first_query_time - second_query_time\n",
    "            \n",
    "            print(f\"\\nðŸ“Š PERFORMANCE COMPARISON\")\n",
    "            print(\"=\" * 50)\n",
    "            print(f\"First Query (no cache): {first_query_time:.2f} seconds\")\n",
    "            print(f\"Second Query (cached): {second_query_time:.3f} seconds\")\n",
    "            print(f\"\\nðŸš€ Speed Improvement: {speedup:.0f}x faster\")\n",
    "            print(f\"â±ï¸ Time Saved: {time_saved:.2f} seconds\")\n",
    "            \n",
    "            # Verify answers are identical\n",
    "            if first_answer == second_answer:\n",
    "                print(f\"\\nâœ“ Answers are identical (cache working correctly)\")\n",
    "            else:\n",
    "                print(f\"\\nâš  Answers differ (cache may not be active)\")\n",
    "            \n",
    "            if speedup > 50:\n",
    "                print(f\"\\nðŸŽ‰ Achieved {speedup:.0f}x performance improvement!\")\n",
    "                print(f\"   This demonstrates production-grade caching!\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"\\nâœ— Request failed: {response.status_code}\")\n",
    "        second_query_time = None\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"\\nâœ— Error: {e}\")\n",
    "    second_query_time = None"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
